<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chronology of Algorithms</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    :root {
      --bg: #0B121C;
      --panel: #0F1C2F;
      --card: #132337;
      --border: #1F4060;
      --blue: #4B8BBE;
      --gold: #FFD43B;
      --green: #4ADE80;
      --red: #F87171;
      --purple: #A78BFA;
      --text: #E2E8F0;
      --soft: #CBD5E1;
      --muted: #94A3B8;
    }
    body {
      background: var(--bg);
      color: var(--text);
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      line-height: 1.55;
      padding: 1.2rem 0.8rem;
    }
    .container {
      max-width: 1380px;
      margin: 0 auto;
      background: rgba(15, 28, 47, 0.75);
      backdrop-filter: blur(6px);
      border: 1px solid var(--border);
      border-radius: 1.4rem;
      padding: 1.6rem 1.8rem;
      box-shadow: 0 24px 48px -16px #000;
    }
    .page-header {
      display: flex;
      align-items: flex-start;
      gap: 1.2rem;
      margin-bottom: 1.4rem;
      border-left: 6px solid var(--gold);
      padding-left: 1.2rem;
    }
    .page-header h1 {
      font-size: 1.9rem;
      color: var(--blue);
      font-weight: 800;
      text-transform: uppercase;
      letter-spacing: -0.02em;
      line-height: 1.15;
    }
    .page-header p {
      font-size: 0.78rem;
      color: var(--soft);
      margin-top: 0.25rem;
    }
    .stats-bar {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 0.8rem;
      margin-bottom: 1.4rem;
    }
    .stat-pill {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 0.8rem;
      padding: 0.6rem 0.9rem;
      text-align: center;
    }
    .stat-pill .num { font-size: 1.4rem; font-weight: 800; color: var(--gold); }
    .stat-pill .lbl { font-size: 0.65rem; color: var(--muted); text-transform: uppercase; letter-spacing: 0.8px; }

    .era-header {
      font-size: 0.82rem;
      color: var(--gold);
      text-transform: uppercase;
      letter-spacing: 1.5px;
      font-weight: 700;
      border-bottom: 2px solid var(--border);
      padding-bottom: 0.3rem;
      margin: 1.4rem 0 0.7rem 0;
    }

    .cards-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 0.75rem;
      margin-bottom: 0.5rem;
    }
    .cards-grid.single { grid-template-columns: 1fr; }

    .timeline-block {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 0.9rem;
      padding: 0.75rem 1rem;
      box-shadow: 0 4px 10px rgba(0,0,0,0.4);
      display: flex;
      flex-direction: column;
      gap: 0.4rem;
      transition: border-color 0.2s;
    }
    .timeline-block:hover { border-color: var(--blue); }

    .block-top {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      flex-wrap: wrap;
    }
    .epoch-marker {
      background: var(--blue);
      color: var(--bg);
      font-weight: 800;
      font-size: 0.65rem;
      padding: 0.15rem 0.65rem;
      border-radius: 20px;
      letter-spacing: 0.4px;
      white-space: nowrap;
    }
    .block-title {
      font-size: 0.88rem;
      color: var(--text);
      font-weight: 700;
    }
    .block-author {
      font-size: 0.68rem;
      color: var(--blue);
      margin-left: auto;
      font-style: italic;
    }
    .epoch-text {
      font-size: 0.76rem;
      color: var(--soft);
      line-height: 1.5;
      text-align: justify;
    }

    .meta-row {
      display: flex;
      gap: 0.4rem;
      flex-wrap: wrap;
      margin-top: 0.1rem;
    }
    .tag {
      font-size: 0.6rem;
      padding: 0.1rem 0.5rem;
      border-radius: 10px;
      font-weight: 600;
      letter-spacing: 0.4px;
      text-transform: uppercase;
    }
    .tag-complexity { background: rgba(75,139,190,0.2); color: var(--blue); border: 1px solid rgba(75,139,190,0.35); }
    .tag-domain   { background: rgba(167,139,250,0.15); color: var(--purple); border: 1px solid rgba(167,139,250,0.3); }
    .tag-impact   { background: rgba(255,212,59,0.12); color: var(--gold); border: 1px solid rgba(255,212,59,0.3); }
    .tag-type     { background: rgba(74,222,128,0.12); color: var(--green); border: 1px solid rgba(74,222,128,0.3); }

    .insight {
      font-size: 0.68rem;
      color: var(--muted);
      border-left: 3px solid var(--border);
      padding-left: 0.5rem;
      font-style: italic;
    }
    .insight span { color: var(--gold); font-weight: 600; font-style: normal; }

    footer {
      margin-top: 2rem;
      text-align: center;
      border-top: 1px solid var(--border);
      padding-top: 1rem;
      font-size: 0.68rem;
      color: var(--soft);
    }
    .gold-accent { color: var(--gold); }

    @media (max-width: 768px) {
      .cards-grid { grid-template-columns: 1fr; }
      .stats-bar { grid-template-columns: repeat(2, 1fr); }
    }
  </style>
</head>
<body>
<div class="container">

  <div class="page-header">
    <div>
      <h1>âš™ï¸ Chronology of Algorithms</h1>
      <p>From ancient clay tablets to neural networks â€” the complete historical timeline arranged in strict chronological order.<br>Each entry includes complexity class, domain, impact level, and key insight.</p>
    </div>
  </div>

  <div class="stats-bar">
    <div class="stat-pill"><div class="num">~3,800</div><div class="lbl">Years Covered</div></div>
    <div class="stat-pill"><div class="num">35+</div><div class="lbl">Landmark Algorithms</div></div>
    <div class="stat-pill"><div class="num">8</div><div class="lbl">Historical Eras</div></div>
    <div class="stat-pill"><div class="num">âˆ</div><div class="lbl">Complexity of Ideas</div></div>
  </div>

  <div class="era-header">ğŸ“œ Antiquity Â· Before 500 CE</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">~1800 BCE</span>
        <span class="block-title">Babylonian Root Extraction</span>
        <span class="block-author">Babylonian scribes</span>
      </div>
      <p class="epoch-text">Clay tablet YBC 7289 shows âˆš2 computed to 6 decimal places using an iterative method equivalent to Heron's algorithm. The oldest known numerical algorithmâ€”a converging sequence, not a formula.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">Complexity: O(k) iterations</span>
        <span class="tag tag-domain">Numerical</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Foundational</span>
        <span class="tag tag-type">Iterative</span>
      </div>
      <div class="insight"><span>Key Insight:</span> Convergence via successive approximation â€” the core idea behind Newton's method millennia later.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">~300 BCE</span>
        <span class="block-title">Euclid's Algorithm (GCD)</span>
        <span class="block-author">Euclid of Alexandria</span>
      </div>
      <p class="epoch-text">Book VII of Elements describes the subtraction-based method for Greatest Common Divisor â€” the oldest algorithm still in widespread daily use. The modular form runs in O(log min(a,b)) steps.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(log min(a,b))</span>
        <span class="tag tag-domain">Number Theory</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Essential</span>
        <span class="tag tag-type">Recursive / Divisive</span>
      </div>
      <div class="insight"><span>Modern Use:</span> Cryptography (RSA key generation), fraction reduction, polynomial GCD in CAS systems.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">~250 BCE</span>
        <span class="block-title">Sieve of Eratosthenes</span>
        <span class="block-author">Eratosthenes of Cyrene</span>
      </div>
      <p class="epoch-text">First known sieve algorithm: mark composite multiples, leaving primes untouched. Runs in O(n log log n) time with O(n) space â€” still competitive for finding all primes below ~10â¹.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n log log n)</span>
        <span class="tag tag-domain">Primality</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Major</span>
        <span class="tag tag-type">Filtering / Sieve</span>
      </div>
      <div class="insight"><span>First space-time tradeoff:</span> Use memory to avoid redundant computation â€” a principle central to dynamic programming.</div>
    </div>

  </div>

  <div class="era-header">ğŸ•¯ï¸ Medieval & Islamic Golden Age Â· 500â€“1500</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">~825 CE</span>
        <span class="block-title">Al-Khwarizmi & Systematic Algebra</span>
        <span class="block-author">Muhammad ibn MÅ«sÄ al-KhwÄrizmÄ«</span>
      </div>
      <p class="epoch-text">"KitÄb al-mukhtaá¹£ar fÄ« á¸¥isÄb al-jabr wal-muqÄbalah" gives the word <em>algebra</em> and, from his Latinized name, <em>algorithm</em> itself. Describes step-by-step methods for linear and quadratic equations â€” procedural thinking formalized.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(1) closed-form steps</span>
        <span class="tag tag-domain">Algebra</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Procedural</span>
      </div>
      <div class="insight"><span>Legacy:</span> The concept that a mathematical procedure could be mechanically followed, regardless of who performs it, is the birth of the algorithm as an idea.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">~1202</span>
        <span class="block-title">Fibonacci's Liber Abaci</span>
        <span class="block-author">Leonardo of Pisa (Fibonacci)</span>
      </div>
      <p class="epoch-text">Introduced Hindu-Arabic numerals and positional notation to Europe, enabling efficient arithmetic algorithms. Described multiplication, division, and the Fibonacci sequence (known in India since ~200 BCE via Pingala's Sanskrit prosody).</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) for Fibonacci</span>
        <span class="tag tag-domain">Arithmetic</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Transformative</span>
        <span class="tag tag-type">Recurrence</span>
      </div>
      <div class="insight"><span>Hidden Pattern:</span> Fibonacci ratios converge to Ï† (golden ratio) â€” appears in nature (phyllotaxis), art, and algorithm analysis (Euclid's worst case).</div>
    </div>

  </div>

  <div class="era-header">ğŸ”­ Renaissance & Enlightenment Â· 1500â€“1800</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1614</span>
        <span class="block-title">Napier's Logarithms</span>
        <span class="block-author">John Napier</span>
      </div>
      <p class="epoch-text">Published log tables transforming multiplication into addition â€” effectively the slide rule's algorithm. Reduced months of astronomical computation to hours. First "hardware acceleration" via pre-computed lookup tables.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(1) table lookup</span>
        <span class="tag tag-domain">Computation</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Revolutionary</span>
        <span class="tag tag-type">Reduction / Transform</span>
      </div>
      <div class="insight"><span>Transform Principle:</span> Convert hard problem â†’ easy problem â†’ invert. The same idea powers FFT (multiplication via log-domain convolution).</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1671â€“1676</span>
        <span class="block-title">Leibniz & Binary Arithmetic</span>
        <span class="block-author">Gottfried Wilhelm Leibniz</span>
      </div>
      <p class="epoch-text">Leibniz independently develops binary numeral system and describes a mechanical calculator using pinwheels. His "Stepped Reckoner" is the first machine designed for all four arithmetic operations â€” encoding computation in physical state.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) for n-bit ops</span>
        <span class="tag tag-domain">Computer Architecture</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Foundational</span>
        <span class="tag tag-type">Encoding / Logic</span>
      </div>
      <div class="insight"><span>Why Binary:</span> Boolean algebra maps perfectly to physical on/off states â€” Leibniz's intuition enabled Shannon's 1937 thesis and all digital logic.</div>
    </div>

  </div>

  <div class="era-header">ğŸ§® 19th Century Â· Algorithm Theory Emerges</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1805</span>
        <span class="block-title">Least Squares Method</span>
        <span class="block-author">Legendre & Gauss</span>
      </div>
      <p class="epoch-text">First statistical parameter-estimation algorithm â€” minimizes sum of squared residuals. Legendre publishes first; Gauss claims priority (1794). Closed-form: Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€y. Foundation of regression, signal processing, and modern ML loss functions.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ·pÂ²) standard form</span>
        <span class="tag tag-domain">Statistics / Optimization</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Essential</span>
        <span class="tag tag-type">Optimization</span>
      </div>
      <div class="insight"><span>Modern Descendant:</span> MSE loss in neural networks; linear regression; GPS positioning; Kalman filtering.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1837 / 1843</span>
        <span class="block-title">Babbage's Engine & Ada's Algorithm</span>
        <span class="block-author">Charles Babbage & Ada Lovelace</span>
      </div>
      <p class="epoch-text">Babbage's Analytical Engine introduces sequential control, branching, and loops. Ada Lovelace writes the first published algorithm for a computing machine â€” a program to compute Bernoulli numbers, including the first loop construct and subroutine call.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">Turing-complete</span>
        <span class="tag tag-domain">Computer Science</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Stored-Program</span>
      </div>
      <div class="insight"><span>First Bug-Free Loop:</span> Lovelace noted the machine could repeat operations â€” predating the concept of iteration by a century of implementation.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1843</span>
        <span class="block-title">Gaussâ€“Seidel Iteration</span>
        <span class="block-author">C.F. Gauss & Philipp Ludwig von Seidel</span>
      </div>
      <p class="epoch-text">Iterative solver for linear systems Ax=b: update each variable in turn using latest values. Guaranteed convergence for diagonally dominant matrices. Predecessor to modern iterative solvers (GMRES, conjugate gradient).</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ²) per iteration</span>
        <span class="tag tag-domain">Numerical Methods</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Major</span>
        <span class="tag tag-type">Iterative Relaxation</span>
      </div>
      <div class="insight"><span>Still Used:</span> Multigrid solvers in CFD, electrical circuit simulation, and Google's PageRank power iteration.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1890</span>
        <span class="block-title">Hollerith Tabulator</span>
        <span class="block-author">Herman Hollerith</span>
      </div>
      <p class="epoch-text">Punched-card sorting and tabulation algorithm for the 1890 US Census â€” completed in 2 years vs. 8 for 1880. Hollerith founded a company that became IBM. First large-scale data-processing algorithm with mechanical execution.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) linear scan</span>
        <span class="tag tag-domain">Data Processing</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Pivotal</span>
        <span class="tag tag-type">Counting Sort (mechanical)</span>
      </div>
      <div class="insight"><span>Birth of Big Data:</span> Proved that algorithmic data processing could outperform human clerks at scale â€” the origin story of IBM.</div>
    </div>

  </div>

  <div class="era-header">ğŸ“ 1900â€“1940 Â· Pre-Electronic Algorithms</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1913</span>
        <span class="block-title">Rungeâ€“Kutta Methods</span>
        <span class="block-author">Carl Runge & Martin Kutta</span>
      </div>
      <p class="epoch-text">Family of explicit numerical integrators for ODEs. RK4 achieves O(hâ´) local error using four function evaluations â€” balancing accuracy and cost. Still the backbone of physics simulation, game engines, and spacecraft trajectory planning.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ·k) per step</span>
        <span class="tag tag-domain">Numerical ODE</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Ubiquitous</span>
        <span class="tag tag-type">Numerical Integration</span>
      </div>
      <div class="insight"><span>Why RK4:</span> Error âˆ hâµ but only 4 evaluations per step â€” sweet spot between Euler (cheap, inaccurate) and higher-order (expensive).</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1936</span>
        <span class="block-title">Turing Machine & Lambda Calculus</span>
        <span class="block-author">Alan Turing & Alonzo Church</span>
      </div>
      <p class="epoch-text">Turing's paper "On Computable Numbers" defines computability via an abstract tape machine. Church independently defines computability via lambda calculus. Churchâ€“Turing thesis: both models capture all effectively computable functions â€” the theoretical bedrock of CS.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">Defines computability itself</span>
        <span class="tag tag-domain">Theory of Computation</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Formal Model</span>
      </div>
      <div class="insight"><span>Halting Problem:</span> Turing simultaneously proved the limits of algorithms â€” some problems are provably undecidable, no matter how fast your hardware.</div>
    </div>

  </div>

  <div class="era-header">ğŸ’¡ 1940â€“1960 Â· First Electronic Algorithms</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1945</span>
        <span class="block-title">von Neumann Architecture & Merge Sort</span>
        <span class="block-author">John von Neumann</span>
      </div>
      <p class="epoch-text">Stored-program concept: instructions and data share memory, enabling self-modifying programs. Von Neumann also invents merge sort (O(n log n)) â€” the first theoretically optimal general-purpose sort, used internally at Los Alamos for the ENIAC.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n log n) merge sort</span>
        <span class="tag tag-domain">Architecture / Sorting</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Foundational</span>
        <span class="tag tag-type">Divide & Conquer</span>
      </div>
      <div class="insight"><span>Divide & Conquer pattern:</span> Split â†’ solve sub-problems â†’ merge results. Pattern also powers FFT, Strassen, and binary search.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1947</span>
        <span class="block-title">Monte Carlo Method</span>
        <span class="block-author">Ulam, von Neumann & Metropolis</span>
      </div>
      <p class="epoch-text">Statistical sampling algorithms using random numbers to approximate solutions to deterministic problems â€” pioneered for neutron diffusion at Los Alamos. Error âˆ 1/âˆšn regardless of dimensionality â€” crucial for high-dimensional integration.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) samples, O(1/âˆšn) error</span>
        <span class="tag tag-domain">Probabilistic</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Transformative</span>
        <span class="tag tag-type">Randomized Sampling</span>
      </div>
      <div class="insight"><span>Dimension-free convergence:</span> Unlike quadrature (error âˆ h^(2/d)), Monte Carlo doesn't suffer the curse of dimensionality.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1951</span>
        <span class="block-title">Simplex Method</span>
        <span class="block-author">George Dantzig</span>
      </div>
      <p class="epoch-text">Walks the vertices of the feasible polytope to maximize a linear objective. Worst-case exponential but typical-case polynomial â€” solved problems from production planning to airline scheduling. One of the most influential algorithms of the 20th century.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">Worst O(2â¿), typical O(mÂ·n)</span>
        <span class="tag tag-domain">Linear Programming</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Essential</span>
        <span class="tag tag-type">Optimization</span>
      </div>
      <div class="insight"><span>Still King:</span> Despite Karmarkar's polynomial-time interior-point rival (1984), simplex remains faster in practice for most LP instances.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1956â€“59</span>
        <span class="block-title">Dijkstra's Shortest Path</span>
        <span class="block-author">Edsger W. Dijkstra</span>
      </div>
      <p class="epoch-text">Greedy single-source shortest path for non-negative weighted graphs. Classic binary-heap version: O((V+E) log V). Conceived while grocery shopping in Amsterdam with his fiancÃ©e â€” published 1959 in 3 pages, no pseudocode, no references.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O((V+E) log V)</span>
        <span class="tag tag-domain">Graph Theory</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Ubiquitous</span>
        <span class="tag tag-type">Greedy / Priority Queue</span>
      </div>
      <div class="insight"><span>Everywhere:</span> GPS navigation, IP routing (OSPF), VLSI layout, network flow decomposition, video game pathfinding (A* = Dijkstra + heuristic).</div>
    </div>

  </div>

  <div class="era-header">âš™ï¸ 1960â€“1970 Â· Complexity & Classic Data Structures</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1960</span>
        <span class="block-title">Quicksort</span>
        <span class="block-author">C.A.R. (Tony) Hoare</span>
      </div>
      <p class="epoch-text">Partition-exchange sort with expected O(n log n) and O(nÂ²) worst case (avoidable with randomized pivot). Cache-friendly, in-place, and small constant factor â€” still the fastest comparison sort in practice. Hoare invented it at age 26 while on a student exchange in Moscow.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n log n) avg, O(nÂ²) worst</span>
        <span class="tag tag-domain">Sorting</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Standard</span>
        <span class="tag tag-type">Divide & Conquer</span>
      </div>
      <div class="insight"><span>Why it wins:</span> Cache locality during partition is orders of magnitude faster than merge sort's auxiliary memory accesses on modern CPUs.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1965</span>
        <span class="block-title">Fast Fourier Transform (FFT)</span>
        <span class="block-author">James Cooley & John Tukey</span>
      </div>
      <p class="epoch-text">Reduces DFT from O(nÂ²) to O(n log n) using divide-and-conquer on the twiddle factors. Gauss had a similar idea in 1805 (unpublished). Called "the most important numerical algorithm of our lifetime" by Gilbert Strang â€” enabled digital audio, MRI, radar, 5G.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n log n)</span>
        <span class="tag tag-domain">Signal Processing</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Transformative</span>
        <span class="tag tag-type">Divide & Conquer</span>
      </div>
      <div class="insight"><span>Secret:</span> FFT also enables O(n log n) polynomial multiplication â€” used in big-integer arithmetic (Python's int * int) and convolution layers.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1962</span>
        <span class="block-title">Redâ€“Black Tree & Hash Table</span>
        <span class="block-author">Bayer (RB-tree) Â· Luhn (hash, 1953)</span>
      </div>
      <p class="epoch-text">Self-balancing BST guaranteeing O(log n) worst-case for all operations. Hash tables provide expected O(1) via hashing + chaining/open addressing. Together they form the two most important dynamic dictionary structures underpinning all databases and language runtimes.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(log n) RB Â· O(1) avg hash</span>
        <span class="tag tag-domain">Data Structures</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Foundational</span>
        <span class="tag tag-type">Balanced Tree / Hashing</span>
      </div>
      <div class="insight"><span>Every Language:</span> Java TreeMap, C++ std::map, Python dict (hash table), Linux kernel's task scheduler all use these structures.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1968</span>
        <span class="block-title">Knuth's TAOCP & Big-O Notation</span>
        <span class="block-author">Donald Knuth</span>
      </div>
      <p class="epoch-text">"The Art of Computer Programming" formalizes algorithm analysis, introduces asymptotic (Big-O) notation to CS, and analyzes hundreds of algorithms with mathematical rigor. Volumes 1â€“3 cover fundamental algorithms; Volume 4 (combinatorics) is still being written at age 86.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">All complexities defined here</span>
        <span class="tag tag-domain">Algorithm Theory</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Analysis Framework</span>
      </div>
      <div class="insight"><span>Bill Gates:</span> "If you think you're a really good programmerâ€¦ read Art of Computer Programming. You should definitely send me a resume if you can read the whole thing."</div>
    </div>

  </div>

  <div class="era-header">ğŸ” 1970â€“1980 Â· NP-Completeness & Public-Key Cryptography</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1971</span>
        <span class="block-title">Cookâ€“Levin Theorem (NP-completeness)</span>
        <span class="block-author">Stephen Cook & Leonid Levin</span>
      </div>
      <p class="epoch-text">Proves SAT is NP-complete â€” every problem in NP reduces to it. Karp (1972) adds 21 more NP-complete problems (TSP, vertex cover, etc.). Defines the P vs NP question: the deepest unsolved problem in mathematics and CS. A $1M Millennium Prize problem.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">NP-complete (hardest in NP)</span>
        <span class="tag tag-domain">Complexity Theory</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Reduction / Classification</span>
      </div>
      <div class="insight"><span>Practical Impact:</span> Tells us which problems to stop trying to solve exactly (TSP) and which to approximate â€” guides algorithm design philosophy.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1977</span>
        <span class="block-title">RSA Cryptosystem</span>
        <span class="block-author">Rivest, Shamir & Adleman</span>
      </div>
      <p class="epoch-text">First practical public-key cryptosystem: encrypt with public key (n,e), decrypt with private key (n,d). Security rests on integer factorization hardness. HTTPS, SSH, PGP, digital signatures â€” the cryptographic backbone of the internet. Key sizes now 2048â€“4096 bits.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(kÂ³) for k-bit key ops</span>
        <span class="tag tag-domain">Cryptography</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Essential</span>
        <span class="tag tag-type">Asymmetric / Number Theory</span>
      </div>
      <div class="insight"><span>Quantum Threat:</span> Shor's algorithm (1994) can factor in polynomial time on a quantum computer â€” driving post-quantum cryptography standards (NIST PQC 2024).</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1976</span>
        <span class="block-title">Knuthâ€“Morrisâ€“Pratt (KMP) String Search</span>
        <span class="block-author">Knuth, Morris & Pratt</span>
      </div>
      <p class="epoch-text">Linear O(n+m) string matching using a precomputed failure function to avoid backtracking. First algorithm to achieve optimal string search. Boyer-Moore (1977) is faster in practice; combined approaches power grep, text editors, antivirus scanning.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n+m) preprocessing + search</span>
        <span class="tag tag-domain">String Processing</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Major</span>
        <span class="tag tag-type">Automaton-based</span>
      </div>
      <div class="insight"><span>Pattern:</span> Failure function embeds the pattern's self-similarity â€” an early use of dynamic programming in string algorithms.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1975</span>
        <span class="block-title">Genetic Algorithms</span>
        <span class="block-author">John Holland</span>
      </div>
      <p class="epoch-text">Evolutionary computation: encode solutions as "chromosomes", apply selection, crossover, and mutation operators to evolve optimal solutions. First major bio-inspired algorithm. Descendants include evolution strategies, differential evolution, and CMA-ES â€” widely used for hyperparameter optimization.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">Varies; O(popÂ·genÂ·eval)</span>
        <span class="tag tag-domain">Evolutionary Computation</span>
        <span class="tag tag-impact">â˜…â˜…â˜… Notable</span>
        <span class="tag tag-type">Stochastic / Bio-inspired</span>
      </div>
      <div class="insight"><span>Modern Form:</span> Neuroevolution of Augmenting Topologies (NEAT) uses GAs to evolve neural network architectures â€” popularized by OpenAI's evolution strategies.</div>
    </div>

  </div>

  <div class="era-header">ğŸ”„ 1980â€“1990 Â· Randomization, Parallelism & Neural Nets</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1987</span>
        <span class="block-title">Backpropagation (Popularized)</span>
        <span class="block-author">Rumelhart, Hinton & Williams</span>
      </div>
      <p class="epoch-text">Efficient O(n) gradient computation via reverse-mode automatic differentiation â€” the chain rule applied to neural network layers. Originally described by Werbos (1974) and Linnainmaa (1970), but Rumelhart et al.'s 1986 Nature paper demonstrated it scaled. Foundation of all deep learning.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) backward pass</span>
        <span class="tag tag-domain">Machine Learning</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining</span>
        <span class="tag tag-type">Automatic Differentiation</span>
      </div>
      <div class="insight"><span>Auto-diff Insight:</span> Computing the gradient costs only ~3Ã— the forward pass regardless of network size â€” enables trillion-parameter models today.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1983</span>
        <span class="block-title">Karmarkar's Interior-Point Method</span>
        <span class="block-author">Narendra Karmarkar</span>
      </div>
      <p class="epoch-text">First provably polynomial-time LP algorithm competitive with simplex in practice. Traverses interior of feasible polytope rather than edges. Sparked interior-point revolution â€” now used in semidefinite programming, second-order cone programming, and convex optimization solvers (CVXOPT, Mosek, Gurobi).</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n^3.5 L)</span>
        <span class="tag tag-domain">Optimization</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Major</span>
        <span class="tag tag-type">Interior-Point</span>
      </div>
      <div class="insight"><span>Breakthrough:</span> Proved for the first time that LP was tractable in theory AND practice â€” Ellipsoid method (Khachian 1979) was polynomial but impractical.</div>
    </div>

  </div>

  <div class="era-header">ğŸŒ 1990â€“2000 Â· Web Scale & Modern Cryptography</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1996â€“98</span>
        <span class="block-title">PageRank</span>
        <span class="block-author">Larry Page & Sergey Brin (Stanford)</span>
      </div>
      <p class="epoch-text">Models the web as a directed graph; rank = stationary distribution of a random surfer with teleportation. Eigenvalue problem: r = dÂ·Ar + (1-d)/n. Computed via power iteration â€” converges in ~50 iterations. Founded Google; worth ~$2 trillion.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(kÂ·|E|) power iteration</span>
        <span class="tag tag-domain">Graph / Search</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Industry-defining</span>
        <span class="tag tag-type">Eigenvector Centrality</span>
      </div>
      <div class="insight"><span>Insight:</span> A link from a high-PageRank page is worth more than many links from low-rank pages â€” recursive prestige, not just link count.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1992</span>
        <span class="block-title">UTF-8 Encoding Algorithm</span>
        <span class="block-author">Ken Thompson & Rob Pike</span>
      </div>
      <p class="epoch-text">Variable-width encoding: ASCII remains 1 byte; Unicode up to 4 bytes with self-synchronizing property (any byte's high bits indicate its role). Designed on a diner placemat in one evening. Now used in >98% of web pages â€” the universal text encoding algorithm.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) decode/encode</span>
        <span class="tag tag-domain">Encoding</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Universal</span>
        <span class="tag tag-type">Variable-Width Coding</span>
      </div>
      <div class="insight"><span>Elegance:</span> Self-synchronizing means you can start reading mid-stream and detect character boundaries â€” critical for streaming and error recovery.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1994</span>
        <span class="block-title">SHA-1 Hash Algorithm</span>
        <span class="block-author">NSA / NIST</span>
      </div>
      <p class="epoch-text">160-bit cryptographic hash function using Merkleâ€“DamgÃ¥rd construction with Daviesâ€“Meyer compression. Widely deployed until 2017 collision attack (SHAttered by Google/CWI). Succeeded by SHA-2 family (SHA-256, SHA-512) and SHA-3 (Keccak sponge construction, 2012).</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n) â€” 80 rounds per block</span>
        <span class="tag tag-domain">Cryptography</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Historic</span>
        <span class="tag tag-type">Hash / Compression</span>
      </div>
      <div class="insight"><span>Deprecated 2017:</span> SHAttered attack produced two PDFs with same SHA-1 hash in 6,500 CPU-years â€” illustrates why hash strength must grow with Moore's Law.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">1996</span>
        <span class="block-title">Inverted Index (Lucene / Elasticsearch)</span>
        <span class="block-author">Doug Cutting (Apache Lucene)</span>
      </div>
      <p class="epoch-text">Maps terms â†’ sorted posting lists of document IDs enabling O(k) term lookup. Combined with BM25 ranking and TF-IDF weighting. Lucene powers Elasticsearch, Solr, and Wikipedia search. Inverted indexes also underpin vector databases (approximate nearest neighbor + scalar filtering).</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(k) lookup, O(n log n) index</span>
        <span class="tag tag-domain">Information Retrieval</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Ubiquitous</span>
        <span class="tag tag-type">Index / Retrieval</span>
      </div>
      <div class="insight"><span>Modern Extension:</span> Vector search (HNSW, IVF-Flat) layers approximate nearest-neighbor on top of inverted indexes for RAG systems and semantic search.</div>
    </div>

  </div>

  <div class="era-header">ğŸ“Š 2000â€“2010 Â· Modern ML & Distributed Computing</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2001</span>
        <span class="block-title">Random Forests</span>
        <span class="block-author">Leo Breiman</span>
      </div>
      <p class="epoch-text">Ensemble of decision trees trained on bootstrapped data with random feature subsets at each split. Combines bagging (variance reduction) and feature randomization (decorrelation). Remarkably robust: low hyperparameter sensitivity, handles missing data, outputs feature importances.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ·kÂ·p log n) training</span>
        <span class="tag tag-domain">Ensemble Learning</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Standard</span>
        <span class="tag tag-type">Ensemble / Bagging</span>
      </div>
      <div class="insight"><span>Bias-Variance:</span> Individual trees are high-variance; averaging hundreds reduces variance without increasing bias â€” the mathematical magic of ensembles.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2004</span>
        <span class="block-title">MapReduce</span>
        <span class="block-author">Jeffrey Dean & Sanjay Ghemawat (Google)</span>
      </div>
      <p class="epoch-text">Two-phase parallel algorithm: Map (embarrassingly parallel key-value transformation) + Reduce (aggregation by key). Hides distribution, fault tolerance, and load balancing from the programmer. Inspired Hadoop, Spark, Flink â€” the foundation of big-data engineering.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(n/p + sort) p machines</span>
        <span class="tag tag-domain">Distributed Computing</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Transformative</span>
        <span class="tag tag-type">Parallel / Functional</span>
      </div>
      <div class="insight"><span>Insight:</span> Move computation to data rather than data to computation â€” reduces network I/O which is the bottleneck at petabyte scale.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2006</span>
        <span class="block-title">Deep Belief Networks & Contrastive Divergence</span>
        <span class="block-author">Geoffrey Hinton et al.</span>
      </div>
      <p class="epoch-text">Greedy layer-wise pre-training using Restricted Boltzmann Machines (RBMs) solved the vanishing-gradient problem â€” enabling deep networks for the first time. Contrastive Divergence (CD-k) is an approximation to the intractable log-likelihood gradient. Sparked the deep learning revival.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ·layers) per CD step</span>
        <span class="tag tag-domain">Deep Learning</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Revival</span>
        <span class="tag tag-type">Generative / Pre-training</span>
      </div>
      <div class="insight"><span>Turning Point:</span> 2006 Science paper showed deep networks could be trained â€” after the "AI winter" of the 1990s. Set the stage for AlexNet (2012).</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2008</span>
        <span class="block-title">Bitcoin Nakamoto Consensus (Proof-of-Work)</span>
        <span class="block-author">Satoshi Nakamoto</span>
      </div>
      <p class="epoch-text">Distributed consensus without a trusted authority: nodes compete to find hash preimages (SHA-256Â² below a target), creating an economically costly Sybil resistance mechanism. Longest chain wins. First practical Byzantine fault-tolerant algorithm for an adversarial open network.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(difficulty) per block</span>
        <span class="tag tag-domain">Distributed Consensus</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Industry-creating</span>
        <span class="tag tag-type">Consensus / Cryptographic</span>
      </div>
      <div class="insight"><span>Energy Trade-off:</span> PoW's security is literally joules â€” Ethereum switched to Proof-of-Stake (2022), using ~99.9% less energy with cryptoeconomic security instead.</div>
    </div>

  </div>

  <div class="era-header">ğŸ§  2010â€“2020 Â· Deep Learning & Transformers</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2012</span>
        <span class="block-title">AlexNet â€” Deep CNN</span>
        <span class="block-author">Krizhevsky, Sutskever & Hinton</span>
      </div>
      <p class="epoch-text">5 convolutional + 3 fully-connected layers with ReLU activations (prevents vanishing gradients), dropout regularization, and GPU-accelerated training (NVIDIA GTX 580). Won ImageNet with 15.3% top-5 error vs 26.2% runner-up â€” a margin so large it ended the debate about deep learning.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ·kÂ²Â·c) per conv layer</span>
        <span class="tag tag-domain">Computer Vision</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Explosive</span>
        <span class="tag tag-type">CNN / Supervised</span>
      </div>
      <div class="insight"><span>ReLU Insight:</span> max(0,x) costs nothing, doesn't saturate, and empirically outperformed tanh/sigmoid â€” a 1-line change that unlocked deep networks.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2015</span>
        <span class="block-title">ResNet â€” Residual Learning</span>
        <span class="block-author">Kaiming He, Zhang, Ren & Sun (Microsoft)</span>
      </div>
      <p class="epoch-text">Skip connections (identity shortcuts) allow gradients to bypass layers: F(x) + x instead of F(x). Enabled training 152-layer networks â€” 8Ã— deeper than VGG-16. Won all ImageNet 2015 tracks. Residual structure is now universal: used in Transformers, diffusion models, and speech recognition.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(L) gradient path (skip)</span>
        <span class="tag tag-domain">Deep Learning</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Universal</span>
        <span class="tag tag-type">Residual / Skip-Connection</span>
      </div>
      <div class="insight"><span>Gradient Highway:</span> Skip connections ensure âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚FÂ·âˆ‚F/âˆ‚x + 1 â€” the "+1" guarantees gradient flows even through many layers.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2017</span>
        <span class="block-title">Transformer (Self-Attention)</span>
        <span class="block-author">Vaswani, Shazeer, Parmar et al. (Google Brain)</span>
      </div>
      <p class="epoch-text">"Attention Is All You Need" â€” replaces RNNs with multi-head self-attention: Q,K,V matrices with softmax(QKáµ€/âˆšd)Â·V. O(nÂ²Â·d) in sequence length but fully parallelizable. Backbone of BERT, GPT, T5, PaLM, Claude, and every modern LLM. Position encoding enables order awareness without recurrence.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ²Â·d) attention</span>
        <span class="tag tag-domain">NLP / General</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Defining Era</span>
        <span class="tag tag-type">Attention / Parallel</span>
      </div>
      <div class="insight"><span>Scalability:</span> Unlike LSTMs (sequential), Transformers scale perfectly across GPU cores â€” 1000Ã— more compute â†’ 1000Ã— better models (scaling laws).</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2018</span>
        <span class="block-title">BERT â€” Bidirectional Pre-training</span>
        <span class="block-author">Devlin, Chang, Lee & Toutanova (Google)</span>
      </div>
      <p class="epoch-text">Masked Language Modeling (MLM): randomly mask 15% of tokens, train to predict them using full bidirectional context. Pre-train on massive corpus, fine-tune on downstream tasks with a single additional layer. Established the pre-train â†’ fine-tune paradigm dominating NLP since 2018.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(nÂ²) attention, 110Mâ€“340M params</span>
        <span class="tag tag-domain">NLP</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Paradigm Shift</span>
        <span class="tag tag-type">Pre-training / Transfer</span>
      </div>
      <div class="insight"><span>Transfer Learning:</span> BERT's key idea â€” language understanding can be separated from task knowledge; pre-training captures universal linguistics.</div>
    </div>

  </div>

  <div class="era-header">ğŸš€ 2020â€“Present Â· Foundation Models & RLHF</div>
  <div class="cards-grid">

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2020</span>
        <span class="block-title">GPT-3 & Few-Shot Learning</span>
        <span class="block-author">Brown, Mann, Ryder et al. (OpenAI)</span>
      </div>
      <p class="epoch-text">175B-parameter autoregressive language model demonstrating emergent few-shot in-context learning: pass examples in the prompt, no gradient updates needed. Showed capabilities emerge discontinuously with scale â€” sparked the generative AI era. Trained on 300B tokens of filtered web text.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">175B params, O(nÂ²) attention</span>
        <span class="tag tag-domain">Generative AI</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Era-Defining</span>
        <span class="tag tag-type">Autoregressive / Scaling</span>
      </div>
      <div class="insight"><span>Emergence:</span> Abilities like multi-step arithmetic, code generation, and analogical reasoning appear suddenly at scale thresholds â€” unpredicted by pre-scale metrics.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2022</span>
        <span class="block-title">ChatGPT & RLHF</span>
        <span class="block-author">OpenAI (Stiennon, Ouyang et al.)</span>
      </div>
      <p class="epoch-text">Reinforcement Learning from Human Feedback: (1) supervised fine-tuning on demonstrations, (2) train reward model on human preference pairs, (3) optimize policy via PPO against reward model with KL-divergence penalty to prevent mode collapse. Reached 100M users in 2 months â€” fastest ever.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(policyÂ·rollouts) per PPO step</span>
        <span class="tag tag-domain">Alignment / RL</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Societal</span>
        <span class="tag tag-type">RLHF / Fine-tuning</span>
      </div>
      <div class="insight"><span>Alignment insight:</span> Human preferences are easier to elicit than to specify as a reward function â€” let humans rank outputs rather than code objectives.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2020â€“2024</span>
        <span class="block-title">Diffusion Models</span>
        <span class="block-author">Ho, Jain & Abbeel (DDPM) Â· Song et al. (Score SDE)</span>
      </div>
      <p class="epoch-text">Learn to reverse a fixed noising process: iteratively denoise gaussian noise guided by a learned score function âˆ‡log p(x). Powers Stable Diffusion, DALL-E 3, Sora, Udio. Outperforms GANs on diversity and training stability. Classifier-free guidance enables text conditioning without a separate classifier.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(TÂ·n) for T denoising steps</span>
        <span class="tag tag-domain">Generative AI / Vision</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜…â˜… Transformative</span>
        <span class="tag tag-type">Generative / Probabilistic</span>
      </div>
      <div class="insight"><span>Thermodynamics link:</span> Forward = entropy increase (add noise); reverse = entropy decrease guided by learned score â€” non-equilibrium thermodynamics in ML.</div>
    </div>

    <div class="timeline-block">
      <div class="block-top">
        <span class="epoch-marker">2023â€“2024</span>
        <span class="block-title">AlphaGeometry & Reasoning Models</span>
        <span class="block-author">DeepMind Â· OpenAI (o1)</span>
      </div>
      <p class="epoch-text">AlphaGeometry combines a language model with a symbolic deduction engine (Deductive Database + Wu's method) to solve IMO geometry at silver-medal level. OpenAI o1 uses chain-of-thought reinforcement learning â€” extended test-time compute as a new scaling axis beyond parameters and data.</p>
      <div class="meta-row">
        <span class="tag tag-complexity">O(search depth) reasoning</span>
        <span class="tag tag-domain">Reasoning / Neuro-Symbolic</span>
        <span class="tag tag-impact">â˜…â˜…â˜…â˜… Frontier</span>
        <span class="tag tag-type">Hybrid / Test-time Compute</span>
      </div>
      <div class="insight"><span>New Scaling Axis:</span> More thinking time (test-time compute) â†’ better answers, independent of parameter count â€” a shift from train-time to inference-time scaling.</div>
    </div>

  </div>

  <footer>
    <span class="gold-accent">âœ¦</span> Chronological Algorithm History Â· Every Step from Euclid to GPT <span class="gold-accent">âœ¦</span><br>
    Each entry includes: time complexity Â· domain classification Â· impact rating Â· key conceptual insight<br>
    <span style="color:var(--muted)">Built by Glenn Junsay Pansensoy</span>
  </footer>
</div>
</body>
</html>
